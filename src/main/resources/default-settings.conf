// 默认请求头，每次发送请求时都会添加这些请求头
defaultHeaders = {
  //  "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/75.0.3770.100 Safari/537.36",
  //  "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_14_5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/76.0.3809.100 Safari/537.36",
  //  "Accept": "*/*",
  //  "Accept-Language": "zh-CN,zh;q=0.8,en-US;q=0.5,en;q=0.3",
  //  "Accept-Encoding": "",
  //  "Connection": "keep-alive",
}

// 设置代理，如果需要翻墙就要使用代理才能正常访问
//proxy = {
//  host = "134.209.103.207"
//  port = 8888
//}

// 最大线程数
maxThreadNum = 4

// 请求超时时间, 单位为s
connectionTimeout = 10.0

// 并发请求的最大值, 默认最多可以同时发出16个请求
concurrentRequests = 8

// 每一个域名下的并发请求最大值
concurrentRequestsPerDomain = 8

// 随机下载延迟, 对于同一个域名下的请求, 两次请求之间会产生一个0-randomDownloadDelay(s)的延迟
randomDownloadDelay = 0.0

// 爬虫的完整类名，如果调用无参的Engine()构造函数时会读取这个配置文件来生成爬虫对象
spiders = [
]


// 调度器配置
scheduler = {
  //设置指定的去重类，该类需要实现me.tianjx98.Jscrapy.duplicatefilter.DuplicateFilter接口
  //  defaultDupFilter = "me.tianjx98.Jscrapy.duplicatefilter.impl.HashDuplicateFilter"
  defaultDupFilter = "me.tianjx98.Jscrapy.duplicatefilter.impl.BloomDuplicateFilter"
  //设置爬取规则，默认为（true）广度优先爬取，设置为false则是深度优先爬取
  bfs = true
}

// 处理爬虫产生的item，数字越小优先级越高
itemPipelines = {
  //  1: "test.scraper.pixiv.PixivPipeline",
  //  2: "test.scraper.pixiv.PixivImageDownloadPipeline",
  //  3: "test.scraper.novel.NovelPipeline",
  //  4: "test.scraper.ddm.MovieItemPipeline",
}
// 爬虫中间件，处理进入爬虫的响应和爬虫产生的请求
spiderMiddlewares = {
  1: "me.tianjx98.Jscrapy.middleware.spider.impl.DepthSpiderMiddleware",
  2: "me.tianjx98.Jscrapy.middleware.spider.impl.AllowedDomainSpiderMiddleware"
}

// 爬虫深度限制，如果请求的深度超过此限制，请求将会被丢弃
depthLimit = 0

// pixiv的账号密码
//username = ""
//password = ""

// 图片的默认存储路径，me.tianjx98.Jscrapy.pipeline.impl.ImageDownloadPipeline会自动读取该路径
//imageStore = "pixiv/images"